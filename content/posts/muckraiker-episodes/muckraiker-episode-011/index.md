---
url: /muckraikers/e011/
title: Understanding AI World Models w/ Chris Canal
summary: Chris Canal, founder of Equistamp, joins muckrAIkers as our first ever podcast guest to discuss AI risks and the world models that inform them.
date: 2025-01-27
lastmod: 

image:
  caption: 'muckrAIkers Temporary Cover Art'
  preview_only: true
  filename: muckraikers_cover-art.jpg

authors:
  - muckraikers

tags:
  - '"AI"'
  - Interview

categories: 
  - Podcast
---

<div style="text-align: justify">
Chris Canal, founder of EquiStamp, joins muckrAIkers as our first ever podcast guest! In this ~3.5 hour interview, we discuss intelligence vs. competencies, the importance of test-time compute, moving goalposts, the orthogonality thesis, and much more. 

A seasoned software developer, Chris started EquiStamp as a way to improve our current understanding of model failure modes and capabilities in late 2023. Now a key contractor for METR, EquiStamp evaluates the next generation of LLMs from frontier model developers like OpenAI and Anthropic.

{{% callout warning %}}
<a href="https://www.equistamp.com/" target="_blank" rel="noreferrer noopener">EquiStamp</a> is hiring, so if you're interested in a fully remote opportunity with flexible working hours, join the EquiStamp <a href="https://discord.com/invite/tjKApmzndk" target="_blank" rel="noreferrer noopener">Discord server</a> and message Chris directly; oh, and let him know muckrAIkers sent you!
{{% /callout %}}

{{< transistor src="https://share.transistor.fm/e/ca6e9de7" >}}
<div style="font-size: x-small;font-style: italic;padding-left: 2.25rem;">EPISODE RECORDED 2025.01.19</a></div>

</div>


## Chapters

<div style="text-align: left; font-family:monospace;">
00:00:00 ❙ Recording date<br>
00:00:05 ❙ Intro<br>
00:00:29 ❙ Hot off the press<br>
00:02:17 ❙ Introducing Chris Canal<br>
00:19:12 ❙ World/risk models<br>
00:35:21 ❙ Competencies + decision making power<br>
00:42:09 ❙ Breaking models down<br>
01:05:06 ❙ Timelines, test time compute<br>
01:19:17 ❙ Moving goalposts<br>
01:26:34 ❙ Risk management pre-AGI<br>
01:46:32 ❙ Happy endings<br>
01:55:50 ❙ Causal chains<br>
02:04:49 ❙ Appetite for democracy<br>
02:20:06 ❙ Tech-frame based fallacies<br>
02:39:56 ❙ Bringing back real capitalism<br>
02:45:23 ❙ Orthogonality Thesis<br>
03:04:31 ❙ Why we do this<br>
03:15:36 ❙ Equistamp!
</div>


## Links
- [EquiStamp](https://www.equistamp.com)
- Chris's [Twitter](https://x.com/chriscanal4)
- METR [Paper](https://metr.org/AI_R_D_Evaluation_Report.pdf) - RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts
- All Trades [article](https://alltrades.substack.com/p/learning-from-history-preventing) - Learning from History: Preventing AGI Existential Risks through Policy by Chris Canal
- Better Systems [article](https://chriscanal.substack.com/p/the-omega-protocol-another-manhattan?r=2ldxa&utm_campaign=post&utm_medium=web&triedRedirect=true) - The Omega Protocol: Another Manhattan Project

### Superintelligence & Commentary
- Wikipedia [article](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies) - Superintelligence: Paths, Dangers, Strategies by Nick Bostrom
- Reflective Altruism [article](https://reflectivealtruism.com/2024/05/30/against-the-singularity-hypothesis-part-5-bostrom-on-the-singularity/) - Against the singularity hypothesis (Part 5: Bostrom on the singularity)
- Into AI Safety [Interview](https://kairos.fm/intoaisafety/e019/) - Scaling Democracy w/ Dr. Igor Krawczuk


### Referenced Sources
- [Book](https://link.springer.com/book/10.1007/978-3-319-24301-6) - Man-made Catastrophes and Risk Information Concealment: Case Studies of Major Disasters and Human Fallibility
- Artificial Intelligence [Paper](https://www.sciencedirect.com/science/article/pii/S0004370221000862) - Reward is Enough
- Wikipedia [article](https://en.wikipedia.org/wiki/Capital_and_Ideology) - Capital and Ideology by Thomas Piketty
- Wikipedia [article](https://en.wikipedia.org/wiki/Pantheon_(TV_series)) - Pantheon


### LeCun on AGI
- "Won't Happen" - Time [article](https://old.reddit.com/r/singularity/comments/1hp7t2i/yann_lecun_doubles_down_that_agi_wont_happen_in/) - Meta’s AI Chief Yann LeCun on AGI, Open-Source, and AI Risk
- "But if it does, it'll be ~~my research agenda~~ latent state models, which I happen to research" - Meta Platforms [Blogpost](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/) - I-JEPA: The first AI model based on Yann LeCun’s vision for more human-like AI


### Other Sources
- Stanford CS [Senior Project](https://cs191w.stanford.edu/projects/Gu,%20Chenchen_CS191W.pdf) - Timing Attacks on Prompt Caching in Language Model APIs
- TechCrunch [article](https://techcrunch.com/2025/01/15/ai-researcher-francois-chollet-founds-a-new-ai-lab-focused-on-agi/) - AI researcher François Chollet founds a new AI lab focused on AGI
- White House [Fact Sheet](https://www.whitehouse.gov/briefing-room/statements-releases/2025/01/13/fact-sheet-ensuring-u-s-security-and-economic-strength-in-the-age-of-artificial-intelligence/) - Ensuring U.S. Security and Economic Strength in the Age of Artificial Intelligence
- New York Post [article](https://nypost.com/2025/01/15/business/lawyer-drops-meta-over-ceo-mark-zuckerbergs-neo-nazi-madness/) - Bay Area lawyer drops Meta as client over CEO Mark Zuckerberg’s ‘toxic masculinity and Neo-Nazi madness’
- OpenEdition [Academic Review](https://journals.openedition.org/oeconomia/10580) of Thomas Piketty
- Neural Processing Letters [Paper](https://link.springer.com/article/10.1007/s11063-021-10562-2) - A Survey of Encoding Techniques for Signal Processing in Spiking Neural Networks
- BFI [Working Paper](https://bfi.uchicago.edu/working-paper/do-financial-concerns-make-workers-less-productive) - Do Financial Concerns Make Workers Less Productive?
- No Mercy/No Malice [article](https://www.profgalloway.com/how-to-survive-the-next-four-years/) - How to Survive the Next Four Years by Scott Galloway