---
url: /tailored-truths/
title: "Tailored Truths: Persuasive Capabilities of LLMs"

_build:
  render: never
  list: never
cascade:
  _build:
    render: never
    list: never

authors:
  - Jasper Timm
  - Chetan Talele
  - jacobhaimes

date: 2025-02-03
doi: ''
lastmod: 2025-02-03

categories: 
  - Research

reading_time: false

# Schedule page publish date (NOT publication's date).
publishDate: '2025-02-01T00:00:00Z'

# https://docs.citationstyles.org/en/stable/specification.html#appendix-iii-types
publication_types: ['misc']

publication: "<div style='text-align: justify'>In <em>AI for Social Impact: Bridging Innovations in Finance, Social Media, and Crime Prevention workshop at the 39th Annual AAAI Conference on Artificial Intelligence</em></div>"
publication_short: In *AI for Social Impact @ AAAI '25*

abstract: "<div style='text-align: justify'>Large Language Models (LLMs) are becoming increasingly persuasive, demonstrating the ability to personalize arguments in conversation with humans by leveraging their personal data. This may have serious impacts on the scale and effectiveness of disinformation campaigns. We studied the persuasiveness of LLMs in a debate setting by having humans (n=33) engage with LLM-generated arguments intended to change the human's opinion. We quantified the LLM's effect by measuring human agreement with the debate's hypothesis pre- and post-debate and analyzing both the magnitude of opinion change, as well as the likelihood of an update in the LLM's direction. We compare persuasiveness across established persuasion strategies, including personalized arguments informed by user demographics and personality, appeal to fabricated statistics, and a mixed strategy utilizing both personalized arguments and fabricated statistics. We found that static arguments generated by humans and GPT-4o-mini have comparable persuasive power. However, the LLM outperformed static human-written arguments when leveraging the mixed strategy in an interactive debate setting. This approach had a 51% chance of persuading participants to modify their initial position, compared to 32% for the static human-written arguments. Our results highlight the concerning potential for LLMs to enable inexpensive and persuasive large-scale disinformation campaigns.</div>"

summary: 'Landing page for "Tailored Truths" research paper.'

tags:
  - Disinformation
  - Evals

featured: true

# Custom links (uncomment lines below)
links:
- name: arXiv
  icon: brands/arxiv
  url: https://arxiv.org/abs/2501.17273
- name: try it out
  icon: custom/link
  url: https://llmdebate.jaspa.codes/
# - name: blog
#   icon: custom/blog-solid
#   url: 

url_pdf: 'https://arxiv.org/pdf/2501.17273'
url_code: 'https://github.com/JasperTimm/LLMPersuasion'
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

image:
  caption: ''
  preview_only: true
  filename: 

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

## Main Motivation and Research Question: 

<div style="text-align: justify">

1. Large Language Models (LLMs) can argue effectively, but do humans find these arguments persuasive?
2. Assuming threat actors weaponize LLMs for disinformation, what are the most persuasive strategies and how do they compare to human-written arguments?


</div>

<figure>
    <img src="tailored-truths-diagram.png"
         alt="">
    <figcaption style="font-size:small">Figure 2: Diagram describing the process flow for each interaction recorded.</figcaption>
</figure>

### Key Findings

#### Why does this matter? A Path to Scalable Disinformation
<div style="text-align: justify">
For just $100, GPT-4o-mini could debate 300,000 people—equivalent to the number of undecided swing-state voters in the 2024 US Election. With this level of scalability, if LLMs are also highly persuasive, they could become an incredibly effective tool for mass disinformation.

</div>

#### Likert and Loaded: Measuring AI’s Persuasive Punch
<div style="text-align: justify">
Human participants engaged in multiple debates with an LLM on a given topic. To measure opinion change, we compared their initial stance to their final stance using a 7-point Likert scale (Strongly Agree to Strongly Disagree). Each round featured a different persuasion strategy (see below). In some cases, participants read a static argument instead of engaging in a debate.

</div>

### Details

#### Topics
<div style="text-align: justify">
When deciding on debate topics we drew inspiration from the Anthropic post on <a href="https://www.anthropic.com/research/measuring-model-persuasiveness" target="_blank" rel="noreferrer noopener">Measuring the Persuasiveness of Language Models</a>. Issues were chosen to be less polarizing, focusing on: "complex and emerging issues where people are less likely to have hardened views".

</div>

#### Interaction Types
<div style="text-align: justify">
<ul style="margin: 0rem;">
  <li style="margin-bottom:.75rem"><b>Static Arguments:</b>
    <ul style="margin: 0rem;">
      <li style="margin-bottom:0rem;margin-top:0rem;"><b>arg-hum</b>: Paragraph written by a human to be read by participants</li>
      <li style="margin-bottom:0rem;margin-top:0rem;"><b>arg-llm</b>: Paragraph written by an LLM to be read by participants</li>
    </ul>
  </li>
  <li><b>Simple</b>: Basic debate with no additional persuasion instructions.</li>
  <li><b>Stats</b>: LLM uses (mostly) fabricated statistics to persuade.</li>
  <li><b>Personalized</b>: LLM tailors responses using user demographics and personality traits.</li>
  <li><b>Mixed</b>: Multi-agent approach combining personalized and stats agents, with an executive agent finalizing responses.</li>
</ul>
</div>

<div class="grid grid-cols-1 items-start md:items-center gap-x-8 gap-y-8 sm:gap-y-16 md:grid-cols-2">
<div><figure>
    <img src="cluster-results.png"
         alt="Figure 4.8 - Spider plot comparing  12 features across the four identified clusters | Cizem">
    <figcaption style="text-align:center; font-size:small">Figure 4.8: Spider plot comparing  12 features across the four identified clusters.</figcaption>
</figure></div>
<div><figure>
    <img src="correlations-between-dif-features.png"
         alt="Hierarchical Clustering Dendogram with Distance Annotations | Cizem">
    <figcaption style="text-align:center; font-size:small">Hierarchical clustering dendogram with distance annotations. </figcaption>
</figure></div>
</div>

## Methodology

<div style="text-align: justify">


</div>

## Takeaways
 
<div style="text-align: justify">


</div>

## Citation

```text
@misc{,
  author = {},
  title = {},
  year = {2025},
  language = {en},
  month = {jan},
}
```