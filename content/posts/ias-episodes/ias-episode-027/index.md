---
url: /intoaisafety/e027/
title: 
summary: 
date: 2026-02-01
lastmod: 2026-02-01
draft: true

_build:
  render: never
  list: never
cascade:
  _build:
    render: never
    list: never

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: ias_cover_art.jpeg

authors:
  - intoaisafety

tags:
  - Interview
  - Training & Field-building
  - Funding

categories:
  - Podcast
---

<div style="text-align: justify">
Description

{{< transistor src="" >}}
<div style="font-size: x-small;font-style: italic;padding-left: 2.25rem;">INTERVIEW RECORDED 2026.01.06; ASIDES RECORDED 2026.01.25; <a href="XXX" target="_blank" rel="noreferrer noopener">TRANSCRIPT</a></div>

## Chapters

<div style="text-align: left; font-family:monospace;">
00:00:00 ❙ Intro<br>
</div>

## Links
* [MATS](https://matsprogram.org/apply?utm_source=ias&utm_medium=&utm_campaign=s26)

### Ryan's Writing
* LessWrong [post](https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams) - Talent needs of technical AI safety teams
* LessWrong [post](https://www.lesswrong.com/posts/yw9B5jQazBKGLjize/ai-safety-undervalues-founders) - AI safety undervalues founders
* LessWrong [comment](https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=6dwq7qkszz5dyZ6sj) - Comment permalink with 2025 MATS program details
* LessWrong [post](https://www.lesswrong.com/posts/WGNYAdBrsNjujJaB8/talk-ai-safety-fieldbuilding-at-mats) - Talk: AI Safety Fieldbuilding at MATS
* LessWrong [post](https://www.lesswrong.com/posts/LvswJts75fnpdjAEj/mats-mentor-selection) - MATS Mentor Selection
* LessWrong [post](https://www.lesswrong.com/posts/Yjiw5rnu4mJsYN8Xc/why-i-funded-pibbss-1) - Why I funded PIBBSS
* EA Forum [post](https://forum.effectivealtruism.org/posts/sGwPgwvaL2FkBHsRh/how-mats-addresses-mass-movement-building-concerns) - How MATS addresses mass movement building concerns

### FTX Funding of AI Safety
* LessWrong [blogpost](https://www.lesswrong.com/posts/WGpFFJo2uFe5ssgEb/an-overview-of-the-ai-safety-funding-situation) - An Overview of the AI Safety Funding Situation
* Fortune [article](https://fortune.com/2022/11/15/sam-bankman-fried-ftx-collapse-a-i-safety-research-effective-altruism-debacle/) - Why Sam Bankman-Fried’s FTX debacle is roiling A.I. research
* NY Times [article](https://www.nytimes.com/2022/12/01/technology/sam-bankman-fried-crypto-artificial-intelligence.html) - FTX probes $6.5M in payments to AI safety group amid clawback crusade
* Cointelegraph [article](https://cointelegraph.com/news/crypto-exchange-ftx-subpoena-center-ai-safety-group-bankruptcy-proceedings) - FTX probes $6.5M in payments to AI safety group amid clawback crusade
* FTX Future Fund [article](https://archive.is/JYJJP) - Future Fund June 2022 Update (archive)
* Tracxn [page](https://tracxn.com/d/companies/anthropic/__SzoxXDMin-NK5tKB7ks8yHr6S9Mz68pjVCzFEcGFZ08/funding-and-investors#funding-rounds) - Anthropic Funding and Investors

### Training & Support Programs
* [Catalyze Impact](https://catalyze-impact.org)
* [Seldon Lab](https://seldonlab.com)
* [SPAR](https://sparai.org)
* [BlueDot Impact](https://bluedot.org)
* [YCombinator](https://www.ycombinator.com)
* [Pivotal](https://www.pivotal-research.org)
* [Athena](https://researchathena.org)
* [Astra Fellowship](https://www.constellation.org/programs/astra-fellowship)
* [Horizon Fellowship](https://horizonpublicservice.org/programs/become-a-fellow/)
* [BASE Fellowship](https://www.baseresearch.org/base-fellowship)
* [LASR Labs](https://www.lasrlabs.org)
* [Entrepeneur First](https://www.joinef.com)

### Funding Organizations
* [Coefficient Giving](https://coefficientgiving.org) (previously Open Philanthropy)
* [LTFF](https://funds.effectivealtruism.org/funds/far-future)
* [Longview Philanthropy](https://www.longview.org)
* [Renaissance Philanthropy](https://www.renaissancephilanthropy.org)

### Coworking Spaces
* [LISA](https://www.safeai.org.uk)
* [Mox](https://moxsf.com)
* [Lighthaven](https://lighthaven.space)
* [FAR Labs](https://www.far.ai/programs/far-labs)
* [Constellation](https://www.constellation.org)
* [Collider](https://collider.nyc)
* [NET Office](https://emergingthreat.net)
* [BAISH](https://www.baish.com.ar/en)

### Research Organizations & Startups
* [Atla AI](https://atla-ai.com)
* [Apollo Research](https://www.apolloresearch.ai)
* [Timaeus](https://timaeus.co)
* [RAND CAST](https://www.rand.org/global-and-emerging-risks/centers/ai-security-and-technology.html)
* [CHAI](https://humancompatible.ai)

### Other Sources
* AXRP [website](https://axrp.net) - The AI X-risk Research Podcast


<!-- end of the list -->
</div>
