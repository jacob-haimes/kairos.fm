---
url: /intoaisafety/e027/
title: Scaling AI Safety Through Mentorship w/ Dr. Ryan Kidd
summary: Dr. Ryan Kidd of MATS shares what actually works in AI safety field-building, and highlights key areas for improvement
date: 2026-02-01
lastmod: 2026-02-01

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: intoaisafety-thumbnail_dr-ryan-kidd_landscape.png

authors:
  - intoaisafety

tags:
  - Interview
  - Training & Field-building
  - Funding

categories:
  - Podcast
---

<div style="text-align: justify">

What does it actually take to build a successful AI safety organization? I'm joined by Dr. Ryan Kidd, who has co-lead MATS from a small pilot program to one of the field's premier talent pipelines. In this episode, he reveals the low-hanging fruit in AI safety field-building that most people are missing: the amplifier archetype.

I pushed Ryan on some hard questions, from balancing funder priorities and research independence, to building a robust selection process for both mentors and participants. Whether you're considering a career pivot into AI safety or already working in the field, this conversation offers practical advice on how to actually make an impact.

As part of my effort to make this whole podcasting thing more sustainable, I have created a Kairos.fm [Patreon](https://www.patreon.com/cw/Kairosfm) which includes an extended version of this episode. Supporting gets you access to these extended cuts, as well as other perks in development.

{{< transistor src="https://share.transistor.fm/e/4b4ebed7" >}}
<div style="font-size: x-small;font-style: italic;padding-left: 2.25rem;">INTERVIEW RECORDED 2026.01.06; ASIDES RECORDED 2026.01.25; <a href="https://share.transistor.fm/s/4b4ebed7/transcript.txt" target="_blank" rel="noreferrer noopener">TRANSCRIPT</a></div>

## Chapters

<div style="text-align: left; font-family:monospace;">
00:00:00 ❙ Intro<br>
00:08:16 ❙ Building MATS Post-FTX & Summer of Love<br>
00:13:09 ❙ Balancing Funder Priorities and Research Independence<br>
00:19:44 ❙ The MATS Selection Process<br>
00:33:15 ❙ Talent Archetypes in AI Safety<br>
00:50:22 ❙ Comparative Advantage and Career Capital in AI Safety<br>
01:04:35 ❙ Building the AI Safety Ecosystem<br>
01:15:28 ❙ What Makes a Great AI Safety Amplifier<br>
01:21:44 ❙ Lightning Round Questions<br>
01:30:30 ❙ Final Thoughts & Outro
</div>

## Links
* [MATS](https://matsprogram.org/apply?utm_source=ias&utm_medium=&utm_campaign=s26)

### Ryan's Writing
* LessWrong [post](https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams) - Talent needs of technical AI safety teams
* LessWrong [post](https://www.lesswrong.com/posts/yw9B5jQazBKGLjize/ai-safety-undervalues-founders) - AI safety undervalues founders
* LessWrong [comment](https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=6dwq7qkszz5dyZ6sj) - Comment permalink with 2025 MATS program details
* LessWrong [post](https://www.lesswrong.com/posts/WGNYAdBrsNjujJaB8/talk-ai-safety-fieldbuilding-at-mats) - Talk: AI Safety Fieldbuilding at MATS
* LessWrong [post](https://www.lesswrong.com/posts/LvswJts75fnpdjAEj/mats-mentor-selection) - MATS Mentor Selection
* LessWrong [post](https://www.lesswrong.com/posts/Yjiw5rnu4mJsYN8Xc/why-i-funded-pibbss-1) - Why I funded PIBBSS
* EA Forum [post](https://forum.effectivealtruism.org/posts/sGwPgwvaL2FkBHsRh/how-mats-addresses-mass-movement-building-concerns) - How MATS addresses mass movement building concerns

### FTX Funding of AI Safety
* LessWrong [blogpost](https://www.lesswrong.com/posts/WGpFFJo2uFe5ssgEb/an-overview-of-the-ai-safety-funding-situation) - An Overview of the AI Safety Funding Situation
* Fortune [article](https://fortune.com/2022/11/15/sam-bankman-fried-ftx-collapse-a-i-safety-research-effective-altruism-debacle/) - Why Sam Bankman-Fried’s FTX debacle is roiling A.I. research
* NY Times [article](https://www.nytimes.com/2022/12/01/technology/sam-bankman-fried-crypto-artificial-intelligence.html) - FTX probes $6.5M in payments to AI safety group amid clawback crusade
* Cointelegraph [article](https://cointelegraph.com/news/crypto-exchange-ftx-subpoena-center-ai-safety-group-bankruptcy-proceedings) - FTX probes $6.5M in payments to AI safety group amid clawback crusade
* FTX Future Fund [article](https://archive.is/JYJJP) - Future Fund June 2022 Update (archive)
* Tracxn [page](https://tracxn.com/d/companies/anthropic/__SzoxXDMin-NK5tKB7ks8yHr6S9Mz68pjVCzFEcGFZ08/funding-and-investors#funding-rounds) - Anthropic Funding and Investors

### Training & Support Programs
* [Catalyze Impact](https://catalyze-impact.org)
* [Seldon Lab](https://seldonlab.com)
* [SPAR](https://sparai.org)
* [BlueDot Impact](https://bluedot.org)
* [YCombinator](https://www.ycombinator.com)
* [Pivotal](https://www.pivotal-research.org)
* [Athena](https://researchathena.org)
* [Astra Fellowship](https://www.constellation.org/programs/astra-fellowship)
* [Horizon Fellowship](https://horizonpublicservice.org/programs/become-a-fellow/)
* [BASE Fellowship](https://www.baseresearch.org/base-fellowship)
* [LASR Labs](https://www.lasrlabs.org)
* [Entrepeneur First](https://www.joinef.com)

### Funding Organizations
* [Coefficient Giving](https://coefficientgiving.org) (previously Open Philanthropy)
* [LTFF](https://funds.effectivealtruism.org/funds/far-future)
* [Longview Philanthropy](https://www.longview.org)
* [Renaissance Philanthropy](https://www.renaissancephilanthropy.org)

### Coworking Spaces
* [LISA](https://www.safeai.org.uk)
* [Mox](https://moxsf.com)
* [Lighthaven](https://lighthaven.space)
* [FAR Labs](https://www.far.ai/programs/far-labs)
* [Constellation](https://www.constellation.org)
* [Collider](https://collider.nyc)
* [NET Office](https://emergingthreat.net)
* [BAISH](https://www.baish.com.ar/en)

### Research Organizations & Startups
* [Atla AI](https://atla-ai.com)
* [Apollo Research](https://www.apolloresearch.ai)
* [Timaeus](https://timaeus.co)
* [RAND CAST](https://www.rand.org/global-and-emerging-risks/centers/ai-security-and-technology.html)
* [CHAI](https://humancompatible.ai)

### Other Sources
* AXRP [website](https://axrp.net) - The AI X-risk Research Podcast


<!-- end of the list -->
</div>
