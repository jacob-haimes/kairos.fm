---
url: /intoaisafety/e023/
title: Growing BlueDot's Impact w/ Li-Lian Ang
summary: How does (likely) the biggest AI safety training organization do it? Li-Lian Ang shares their strategy.
date: 2025-09-15
lastmod: 2025-09-15

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: intoaisafety-thumbnail_li-lian-ang_1280x720.jpg

authors:
  - intoaisafety

tags:
  - Interview
  - Advocacy
  - Training/Field-building

categories:
  - Podcast
---

<div style="text-align: justify">
    I'm joined by my good friend, Li-Lian Ang, first hire and product manager at BlueDot Impact. We discuss how BlueDot has evolved from their original course offerings to a new "defense-in-depth" approach, which focuses on three core threat models: reduced oversight in high risk scenarios (e.g. accelerated warfare), catastrophic terrorism (e.g. rogue actors with bioweapons), and the concentration of wealth and power (e.g. supercharged surveillance states). On top of that, we cover how BlueDot's strategies account for and reduce the negative impacts of common issues in AI safety, including exclusionary tendencies, elitism, and echo chambers.<br>

{{% callout warning %}}
2025.09.15: Learn more about how to make design effective interventions to make AI go well and potentially even get funded for it on BlueDot Impact's <a href="https://bluedot.org/courses/agi-strategy" target="_blank" rel="noreferrer noopener">AGI Strategy course</a>! BlueDot is also <a href="https://bluedot.org/join-us" target="_blank" rel="noreferrer noopener">hiring</a>, so if you think you’d be a good fit, I definitely recommend applying; I had a great experience when I contracted as a course facilitator. If you do end up applying, let them know you found out about the opportunity from the podcast!
{{% /callout %}}<br>

Follow Li-Lian on [LinkedIn](https://www.linkedin.com/in/anglilian/), and look at more of her work on her [blog](https://anglilian.com/)!

As part of my effort to make this whole podcasting thing more sustainable, I have created a Kairos.fm [Patreon](https://www.patreon.com/cw/Kairosfm) which includes an extended version of this episode. Supporting gets you access to these extended cuts, as well as other perks in development.

{{< transistor src="https://share.transistor.fm/e/693171f2" >}}
<div style="font-size: x-small;font-style: italic;padding-left: 2.25rem;">INTERVIEW RECORDED 2025.08.27; ASIDES RECORDED 2025.09.07</div>

## Chapters

<div style="text-align: left; font-family:monospace;">
00:00:00 ❙ Intro<br>
00:03:23 ❙ Meeting Through the Course<br>
00:05:46 ❙ Eating Your Own Dog Food<br>
00:13:13 ❙ Impact Acceleration<br>
00:22:13 ❙ Breaking Out of the AI Safety Mold<br>
00:26:06 ❙ Bluedot’s Risk Framework<br>
00:41:38 ❙ Dangers of "Frontier" Models<br>
00:54:06 ❙ The Need for AI Safety Advocates<br>
01:00:11 ❙ Hot Takes and Pet Peeves
</div>

## Links
- BlueDot Impact [website](https://bluedot.org)

### Defense-in-Depth
- BlueDot Impact [blogpost](https://bluedot.org/blog/course-portfolio-vision) - Our vision for comprehensive AI safety training
- Engineering for Humans [blogpost](https://www.engineeringforhumans.com/systems-engineering/the-swiss-cheese-model-designing-to-reduce-catastrophic-losses/) - The Swiss cheese model: Designing to reduce catastrophic losses
- Open Journal of Safety Science and Technology [article](https://www.scirp.org/journal/paperinformation?paperid=70457) - The Evolution of Defense in Depth Approach: A Cross Sectorial Analysis

### X-clusion and X-risk
- Nature [article](https://arxiv.org/abs/2502.09288) - AI Safety for Everyone
- Ben Kuhn [blogpost](https://www.benkuhn.net/welcoming/) - On being welcoming
- Reflective Altruism [blogpost](https://reflectivealtruism.com/2023/01/12/off-series-that-bostrom-email/) - Belonging (Part 1: That Bostrom email)

### AIxBio
- RAND [report](https://www.rand.org/pubs/research_reports/RRA2977-2.html) - The Operational Risks of AI in Large-Scale Biological Attacks
- OpenAI ["publication"](https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/) (press release) - Building an early warning system for LLM-aided biological threat creation
- Anthropic Frontier AI Red Team [blogpost](https://red.anthropic.com/2025/biorisk/) - Why do we take LLMs seriously as a potential source of biorisk?
- Kevin Esvelt [preprint](https://arxiv.org/pdf/2503.15182) - Foundation models may exhibit staged progression in novel CBRN threat disclosure
- Anthropic [press release](https://www.anthropic.com/news/activating-asl3-protections) - Activating AI Safety Level 3 protections

### Persuasive AI
- [Preprint](https://arxiv.org/abs/2412.17128v1) - Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models
- Nature Human Behavior [article](https://www.nature.com/articles/s41562-025-02194-6?error=cookies_not_supported&code=32f522a9-c964-4efa-9565-e013fbef4d63) - On the conversational persuasiveness of GPT-4
- [Preprint](https://arxiv.org/abs/2505.09662) - Large Language Models Are More Persuasive Than Incentivized Human Persuaders

### AI, Anthropomorphization, and Mental Health
- Western News [article](https://news.westernu.ca/2025/08/danger-of-anthropomorphic-ai/) - Expert insight: Humanlike chatbots detract from developing AI for the human good
- AI & Society [article](https://link.springer.com/article/10.1007/s00146-022-01492-1) - Anthropomorphization and beyond: conceptualizing humanwashing of AI-enabled machines
- Artificial Ignorance [article](https://www.ignorance.ai/p/the-chatbot-trap) - The Chatbot Trap
- Making Noise and Hearing Things [blogpost](https://makingnoiseandhearingthings.com/2022/08/03/large-language-models-cannot-replace-mental-health-professionals/) - Large language models cannot replace mental health professionals
- Idealogo [blogpost](https://www.brightfama.com/blog/2025/08/28/4-reasons-not-to-turn-chatgpt-into-your-therapist/) - 4 reasons not to turn ChatGPT into your therapist
- Journal of Medical Society [Editorial](https://journals.lww.com/jmso/fulltext/2024/38010/importance_of_informed_consent_in_medical_practice.1.aspx) - Importance of informed consent in medical practice
- Indian Journal of Medical Research [article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7055160/) - Consent in psychiatry - concept, application & implications
- Media Naama [article](https://www.medianama.com/2025/04/223-chatgpt-sycophantic-tone-risks-humanizing-ai-chatbots/) - The Risk of Humanising AI Chabots: Why ChatGPT Mimicking Feelings Can Backfire
- Becker's Behavioral Health [blogpost](https://www.beckersbehavioralhealth.com/ai-2/openais-mental-health-roadmap-5-things-to-know/) - OpenAI’s mental health roadmap: 5 things to know

### Miscellaneous References
- Carnegie Council [blogpost](https://carnegiecouncil.org/media/article/what-do-we-mean-when-we-talk-about-ai-democratization) - What Do We Mean When We Talk About "AI Democratization"?
- Collective Intelligence Project [policy brief](https://www.cip.org/research/democratizing-ai) - Four Approaches to Democratizing AI
- BlueDot Impact [blogpost](https://bluedot.org/blog/how-does-ai-learn) - How Does AI Learn? A Beginner's Guide with Examples
- BlueDot Impact [blogpost](https://bluedot.org/blog/ai-safety-advocacy) - AI safety needs more public-facing advocacy

### More Li-Lian Links
- Humans of Minerva podcast [website](https://humansofminerva.com)
- Li-Lian's [book](https://www.goodreads.com/book/show/57027582-purple-is-the-noblest-shroud) - Purple is the Noblest Shroud

### Relevant Podcasts from Kairos.fm
- [Scaling Democracy w/ Dr. Igor Krawczuk](https://kairos.fm/intoaisafety/e019/) for AI safety exclusion and echo chambers
- [Getting into PauseAI w/ Will Petillo](https://kairos.fm/intoaisafety/e021/) for AI in warfare and exclusion in AI safety

<!-- end of the list -->
</div>
