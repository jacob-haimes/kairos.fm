---
title: StakeOut.AI w/ Dr. Peter Park (3)
summary: "As always, the best things come in 3s: dimensions, musketeers, pyramids, and... 3 installments of my interview with Dr. Peter Park!"
date: 2024-03-24
lastmod: 2024-06-18

featured: true

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: ias_cover_art.jpeg

authors:
  - intoaisafety

tags:
  - Interview
  - StakeOut.AI Feb 2024
  - AISC

categories: 
  - Podcast
---

<div style="text-align: justify">
As always, the best things come in 3s: dimensions, musketeers, pyramids, and... 3 installments of my interview with Dr. Peter Park, an AI Existential Safety Post-doctoral Fellow working with Dr. Max Tegmark at MIT.

As you may have ascertained from the previous two segments of the interview, Dr. Park cofounded <a href="https://www.stakeout.ai" target="_blank" rel="noreferrer noopener">StakeOut.AI</a> along with Harry Luk and one other cofounder whose name has been removed due to requirements of her current position. The non-profit had a simple but important mission: make the adoption of AI technology go well, for humanity, but unfortunately, StakeOut.AI had to dissolve in late February of 2024 because no granter would fund them. Although it certainly is disappointing that the organization is no longer functioning, all three cofounders continue to contribute positively towards improving our world in their current roles.

{{< transistor src="https://share.transistor.fm/e/8c7eea4d" >}}

If you would like to investigate further into Dr. Park's work, view his <a href="https://scholar.harvard.edu/pspark" target="_blank" rel="noreferrer noopener">website</a>, <a href="https://scholar.google.com/citations?user=5lMAPEoAAAAJ&hl=en" target="_blank" rel="noreferrer noopener">Google Scholar</a>, or follow him on <a href="https://twitter.com/dr_park_phd" target="_blank" rel="noreferrer noopener">Twitter</a>!
</div>

### Chapters

<div style="text-align: left; font-family:monospace;">
00:00:54 ❙ Intro<br>
00:02:41 ❙ Rapid development<br>
00:08:25 ❙ Provable safety, safety factors, & CSAM<br>
00:18:50 ❙ Litigation<br>
00:23:06 ❙ Open/Closed Source<br>
00:38:52 ❙ AIxBio<br>
00:47:50 ❙ Scientific rigor in AI<br>
00:56:22 ❙ AI deception<br>
01:02:45 ❙ No takesies-backsies<br>
01:08:22 ❙ StakeOut.AI's start<br>
01:12:53 ❙ Sustainability & Agency<br>
01:18:21 ❙ "I'm sold, next steps?" -you<br>
01:23:53 ❙ Lessons from the amazing Spiderman<br>
01:33:15 ❙ "I'm ready to switch careers, next steps?" -you<br>
01:40:00 ❙ The most important question<br>
01:41:11 ❙ Outro
</div>

### Links

Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance.
- <a href="https://www.stakeout.ai" target="_blank" rel="noreferrer noopener">StakeOut.AI</a>
- <a href="https://pauseai.info" target="_blank" rel="noreferrer noopener">Pause AI</a>
- <a href="https://futureoflife.org/wp-content/uploads/2023/11/FLI_Governance_Scorecard_and_Framework.pdf" target="_blank" rel="noreferrer noopener">AI Governance Scorecard</a> (go to Pg. 3)
- <a href="https://civitai.com" target="_blank" rel="noreferrer noopener">CIVITAI</a>
  - <a href="https://www.404media.co/a16z-funded-ai-platform-generated-images-that-could-be-categorized-as-child-pornography-leaked-documents-show/" target="_blank" rel="noreferrer noopener">Article on CIVITAI and CSAM</a>
- <a href="https://www.judiciary.senate.gov/protecting-children-online" target="_blank" rel="noreferrer noopener">Senate Hearing: Protecting Children Online</a>
  - <a href="https://www.pbs.org/newshour/politics/watch-live-ceos-of-meta-tiktok-x-and-other-social-media-companies-testify-in-senate-hearing" target="_blank" rel="noreferrer noopener">PBS Newshour Coverage</a>
- <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html" target="_blank" rel="noreferrer noopener">The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work</a>
- Open Source/Weights/Release/Interpretation
  - <a href="https://opensource.org" target="_blank" rel="noreferrer noopener">Open Source Initiative</a>
    - <a href="https://opensource.org/history" target="_blank" rel="noreferrer noopener">History of the OSI</a>
    - <a href="https://opensource.org/blog/metas-llama-2-license-is-not-open-source" target="_blank" rel="noreferrer noopener">Meta’s LLaMa 2 license is not Open Source</a>
  - <a href="https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/" target="_blank" rel="noreferrer noopener">Is Llama 2 open source? No – and perhaps we need a new definition of open…</a>
  - <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="noreferrer noopener">Apache License, Version 2.0</a>
  - <a href="https://www.3blue1brown.com/topics/neural-networks" target="_blank" rel="noreferrer noopener">3Blue1Brown: Neural Networks</a>
  - <a href="https://dl.acm.org/doi/10.1145/3571884.3604316" target="_blank" rel="noreferrer noopener">Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators</a>
    - The online <a href="https://opening-up-chatgpt.github.io" target="_blank" rel="noreferrer noopener">table</a>
- <a href="https://www.signal.org" target="_blank" rel="noreferrer noopener">Signal</a>
- <a href="https://huggingface.co/bigscience/bloomz" target="_blank" rel="noreferrer noopener">Bloomz</a> model on HuggingFace
- <a href="https://mistral.ai" target="_blank" rel="noreferrer noopener">Mistral</a> website
- NASA Tragedies
  - <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster" target="_blank" rel="noreferrer noopener">Challenger disaster</a> on Wikipedia
  - <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Columbia_disaster" target="_blank" rel="noreferrer noopener">Columbia disaster</a> on Wikipedia
- AIxBio Risk
  - <a href="https://www.nature.com/articles/s42256-022-00465-9" target="_blank" rel="noreferrer noopener">Dual use of artificial-intelligence-powered drug discovery</a>
  - <a href="https://arxiv.org/abs/2306.03809" target="_blank" rel="noreferrer noopener">Can large language models democratize access to dual-use biotechnology?</a>
  - <a href="https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models" target="_blank" rel="noreferrer noopener">Open-Sourcing Highly Capable Foundation Models</a> _(sadly, I can't rename the article...)_
  - <a href="https://1a3orn.com/sub/essays-propaganda-or-science.html" target="_blank" rel="noreferrer noopener">Propaganda or Science: Open Source AI and Bioterrorism Risk</a>
  - <a href="https://ineffectivealtruismblog.com/2024/03/09/exaggerating-the-risks-part-14-biorisk-from-llms/" target="_blank" rel="noreferrer noopener">Exaggerating the risks (Part 15: Biorisk from LLMs)</a>
  - <a href="https://arxiv.org/abs/2310.18233" target="_blank" rel="noreferrer noopener">Will releasing the weights of future large language models grant widespread access to pandemic agents?</a>
  - <a href="https://crfm.stanford.edu/open-fms/" target="_blank" rel="noreferrer noopener">On the Societal Impact of Open Foundation Models</a>
    - <a href="https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf" target="_blank" rel="noreferrer noopener">Policy brief</a>
- <a href="https://www.apartresearch.com" target="_blank" rel="noreferrer noopener">Apart Research</a>
- <a href="https://www.science.org" target="_blank" rel="noreferrer noopener">Science</a>
- Cicero
  - <a href="https://www.science.org/doi/10.1126/science.ade9097" target="_blank" rel="noreferrer noopener">Human-level play in the game of Diplomacy by combining language models with strategic reasoning</a>
  - <a href="https://ai.meta.com/research/cicero/" target="_blank" rel="noreferrer noopener">Cicero</a> webpage
  - <a href="https://arxiv.org/abs/2308.14752" target="_blank" rel="noreferrer noopener">AI Deception: A Survey of Examples, Risks, and Potential Solutions</a>
- <a href="https://demos.co.uk/research/open-sourcing-the-ai-revolution-framing-the-debate-on-open-source-artificial-intelligence-and-regulation/" target="_blank" rel="noreferrer noopener">Open Sourcing the AI Revolution: Framing the debate on open source, artificial intelligence and regulation</a>
- <a href="https://aisafety.camp" target="_blank" rel="noreferrer noopener">AI Safety Camp</a>
- <a href="https://www.patreon.com/IntoAISafety" target="_blank" rel="noreferrer noopener">Into AI Safety Patreon</a>

<!-- end of the list -->