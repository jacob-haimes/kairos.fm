---
url: /intoaisafety/e007/
title: Evals Hackathon November 2023 (1)
summary: Recording of my team's first meeting during the Evals Hackathon hosted by Apart Research and Apollo Research in November of 2023.
date: 2023-10-25
lastmod: 2024-06-17

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: ias_cover_art.jpeg

authors:
  - intoaisafety

tags:
  - Hackathon
  - Research
  - Evals November 2023

categories: 
  - Podcast
---

<div style="text-align: justify">
This episode kicks off our first subseries, which will consist of recordings taken during my team's meetings for the AlignmentJams Evals Hackathon in November of 2023. Our team won first place, so you'll be listening to the process which, at the end of the day, turned out to be pretty good.


{{< transistor src="https://share.transistor.fm/e/04e9b1a9" >}}


Check out <a href="https://apartresearch.com" target="_blank" rel="noreferrer noopener">Apart Research</a>, the group that runs the <a href="https://alignmentjam.com" target="_blank" rel="noreferrer noopener">AlignmentJamz Hackathons</a>.
</div>

### Links

Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance.
- <a href="https://arxiv.org/abs/2311.07723" target="_blank" rel="noreferrer noopener">Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains</a>
  - <a href="https://www.lesswrong.com/posts/Yio4nmD8JMttx9o9S/new-paper-shows-truthfulness-and-instruction-following-don-t" target="_blank" rel="noreferrer noopener">New paper shows truthfulness & instruction-following don't generalize by default</a>
  - <a href="https://joshuaclymer.github.io/generalization-analogies-website/" target="_blank" rel="noreferrer noopener">Generalization Analogies Website</a>
- <a href="https://arxiv.org/abs/2212.09251" target="_blank" rel="noreferrer noopener">Discovering Language Model Behaviors with Model-Written Evaluations</a>
  - <a href="https://www.evals.anthropic.com" target="_blank" rel="noreferrer noopener">Model-Written Evals Website</a>
- <a href="https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say" target="_blank" rel="noreferrer noopener">OpenAI Evals GitHub</a>
- <a href="https://metr.org " target="_blank" rel="noreferrer noopener">METR</a> (previously ARC Evals)
- <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" target="_blank" rel="noreferrer noopener">Goodharting on Wikipedia</a>
- <a href="https://arxiv.org/abs/2308.12014" target="_blank" rel="noreferrer noopener">From Instructions to Intrinsic Human Values, a Survey of Alignment Goals for Big Models</a>
- <a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="noreferrer noopener">Fine Tuning Aligned Language Models Compromises Safety Even When Users Do Not Intend</a>
- <a href="https://arxiv.org/abs/2310.02949" target="_blank" rel="noreferrer noopener">Shadow Alignment: The Ease of Subverting Safely Aligned Language Models</a>
- <a href="https://arxiv.org/abs/2310.18233" target="_blank" rel="noreferrer noopener">Will Releasing the Weights of Future Large Language Models Grant Widespread Access to Pandemic Agents?</a>
- <a href="https://www.sciencedirect.com/science/article/pii/S2666389923002210" target="_blank" rel="noreferrer noopener">Building Less Flawed Metrics, Understanding and Creating Better Measurement and Incentive Systems</a>
- <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" rel="noreferrer noopener">eLeutherAI's Model Evaluation Harness</a>
- <a href="https://github.com/danbraunai/evalugator/tree/main" target="_blank" rel="noreferrer noopener">Evalugator Library</a>

<!-- end of the list -->