---
title: AISC Proposal w/ Remmelt Ellen
summary: Discussion of my initial research proposal for the 2024 Winter AI Safety Camp with Remmelt Ellen.
date: 2023-10-25
lastmod: 2024-06-17

image:
  caption: 'Into AI Safety Cover Art'
  preview_only: true
  filename: ias_cover_art.jpeg

authors:
  - intoaisafety

tags:
  - Feedback
  - AISC

categories: 
  - Podcast
---

<div style="text-align: justify">
In this episode I discuss my initial research proposal for the 2024 Winter AI Safety Camp with one of the individuals who helps facilitate the program, Remmelt Ellen.<br>

{{< transistor src="https://share.transistor.fm/e/57154a54" >}}

The proposal is titled "The Effect of Machine Learning on Bioengineered Pandemic Risk". A doc-capsule of the proposal at the time of this recording can be found <a href="https://docs.google.com/document/d/1bbFDNc_hzhzYqN6pn1jYvPhSRp2CyYez4UP0ly_mMRM/edit?usp=sharing" target="_blank" rel="noreferrer noopener">at this link</a>.
</div>

### Links
Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance.
- <a href="https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45" target="_blank" rel="noreferrer noopener">MegaSyn: Integrating Generative Molecule Design, Automated Analog Designer and Synthetic Viability Prediction</a>
- <a href="https://www.nature.com/articles/s42256-022-00465-9?fbclid=IwAR11_V1cd9SUxEvUfwrWMA7TUcroyYIY1nBDUL3KaS-8B4rG5MIqZCmjm0M" target="_blank" rel="noreferrer noopener">Dual use of artificial-intelligence-powered drug discovery</a>
- <a href="https://arxiv.org/abs/2306.13952" target="_blank" rel="noreferrer noopener">Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools</a>
- <a href="https://www.dataguidance.com/news/china-cac-proposes-global-ai-governance-initiative" target="_blank" rel="noreferrer noopener">China CAC Proposes Global AI Governance Initiative</a>
- <a href="https://www.technologyreview.com/2023/10/18/1081846/generative-ai-safety-censorship-china/" target="_blank" rel="noreferrer noopener">China has a new plan for judging the safety of generative AI — and it’s packed with details</a>
- <a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1" target="_blank" rel="noreferrer noopener">Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research</a>
- <a href="https://arxiv.org/abs/2310.02949" target="_blank" rel="noreferrer noopener">Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models</a>
- <a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="noreferrer noopener">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a>
- <a href="https://www.alignmentforum.org/posts/3eqHYxfWb5x4Qfz8C/unrlhf-efficiently-undoing-llm-safeguards" target="_blank" rel="noreferrer noopener">unRLHF - Efficiently undoing LLM safeguards</a>

<!-- end of the list -->